\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,latexsym}
\usepackage{alltt,enumerate}
\usepackage{tikz,pgfmath,algorithm,algorithmic}
\usepgflibrary{shapes}
\usetikzlibrary{arrows,automata,backgrounds}

\usepackage[style=numeric]{biblatex}
\addbibresource{bibtex.bib}

\begin{document}

%% Macros %%
\newcommand\one{\textsf 1}
\newcommand\zero{\textsf 0}
\newcommand\cost{\mathcal C}

%%\newtheorem{remark}{Remark}
%%\newtheorem{question}{Q}
\theoremstyle{remark}
\newtheorem{alg}{Algorithm}

\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}

\setlength\parindent{0in}
\addtolength\parskip{1ex}
\setlength\fboxrule{.5mm}\setlength{\fboxsep}{1.2mm}
\newlength\courseheader
\setlength\courseheader\textwidth
\addtolength\courseheader{-4mm}
\parindent=0pt
\parskip=1ex

\begin{center}
\framebox{\parbox\courseheader{\large
CS6820 Algorithms\hfill December 15, 2021\\
Final Project \hfill G\"oktu\u{g} Saatcioglu (gs724) \hfill Mark Moeller (mdm367)}}
\end{center}
\medskip

These notes explore the problem of sampling random spanning trees on graphs. The
focus will be to present and analyze Wilson's famous algorithm for this problem,
but we take some detours along the way.

We have also implemented the algorithms presented and run them on various
graphs. Details of our implementation are presented in Section~\ref{imp}.
As a 6820 final project, our approach is a hybrid of the ``Extra topic'' and ``Coding
project'' project types.

\section{Introduction: Spanning Trees}

In this section we introduce notation and explore basics of distributions on
spanning trees.

\subsection{Definitions}

\begin{defn}
For a connected undirected graph $G = (V, E)$, a collection of edges $T \subseteq E$
form a \emph{spanning tree} if $(V, T)$ is connected and acyclic.
\end{defn}

Suppose the edges have nonnegative costs given by a function $w\colon E \to
\mathbb{R^+}$, then the weight of a spanning tree is:
\[ w(T) = \prod_{e\in T} w(e) \]

\begin{defn}
A spanning tree for $G$ is called the \emph{minimum spanning tree} if it has the
minimum weight of all spanning trees of $G$.
\end{defn}

Prim's and Kruskal's algorithms are classic fast algorithms for finding minimum
spanning trees. Kozen gives an enlightening general framework that includes both
algorithms \cite{kozen}.

In some situations in distributed systems or networking, however, we may not
want the minimum spanning tree necessarily, but instead we want to sample
randomly from a distribution of spanning trees. We will present an algorithm due
to Wilson \cite{wilson} for doing this in Section~\ref{wilson}. Then we will see
how it can be extended to consider graphs with weights. In this case, the
probability of sampling a given tree will be proportional to its weight.

% Some stuff about how many trees there are?


\section{Naive First Attempt}

We might be tempted to try to sample random spanning trees by modifying
Kruskal's algorithm. That is:
\begin{itemize}
\item Choose a random order on edges, perhaps based on edge weights
\item Choose edges from the list in order, skipping an edge if it would make a
cycle
\end{itemize}

Unfortunately, we will not achieve the appropriate distributions following this
path. For a counterexample, consider the triangle graph $G = (\{a,b,c\},
\{(a,b),(b,c),(a,c)\})$, with edge weights:
\[w(e) = \begin{cases}
        2 & \text{ for } e = (a,b)\\
        1 & \text{ for } e = (b,c)\\
        1 & \text{ for } e = (a,c)
        \end{cases}\]

We observe that for this graph there are 3 spanning trees:
$T_0 = \{(a,b), (a,c)\}$,
$T_1 = \{(a,b), (b,c)\}$,
$T_2 = \{(a,c), (b,c)\}$, with weights:

\[w(T) = \begin{cases}
        2 & \text{ for } T = T_0\\
        2 & \text{ for } T = T_1\\
        1 & \text{ for } T = T_2
        \end{cases}\]

Therefore we want to sample $T_0$ or $T_1$ each with probability 2/5, and $T_2$
with probability 1/5. But if we sample the trees by the method described above
(i.e., choosing edges propropotional to their weight---and for this graph just
pick the first two), then we get the following distribution:

\begin{align*}
(a,b), (a,c)\text{ with Pr }= 1/2 \cdot 1/2 = 1/4, (T_0 \text{ is selected})\\
(a,c), (a,b)\text{ with Pr }= 1/4 \cdot 2/3 = 1/6, (T_0 \text{ is selected})\\
(a,b), (b,c)\text{ with Pr }= 1/2 \cdot 1/2 = 1/4, (T_1 \text{ is selected})\\
(b,c), (a,b)\text{ with Pr }= 1/4 \cdot 2/3 = 1/6, (T_1 \text{ is selected})\\
(b,c), (a,c)\text{ with Pr }= 1/4 \cdot 1/3 = 1/12, (T_2 \text{ is selected})\\
(a,c), (b,c)\text{ with Pr }= 1/4 \cdot 1/3 = 1/12, (T_2 \text{ is selected})
\end{align*}

So we get:
\begin{align*}
\text{Pr}(T_0) = 1/4 + 1/6 = 10/24\\
\text{Pr}(T_1) = 1/4 + 1/6 = 10/24\\
\text{Pr}(T_2) = 1/12 + 1/12 = 1/6
\end{align*}

which is the wrong distribution (see above). We will see in the next section that
\emph{loop-erased random walks} are the key to sampling spanning trees.

\section{Loop-erased Random Walks}

The definitions in this section allow use of directed or undirected graphs. We
assume a stochastic transition matrix. If the edge weights of our graph do not
already form a stochastic matrix, we can obtain one by normalizing the weights
and (possibly) adding self-transitions for slack in probabilities. These
self-transitions will have no affect on the ensuing algorithms, for reasons that
will be clear shortly.

\begin{defn}
A \emph{random walk} on a graph $G=(V,E)$ is a sequence of vertices $v_i \in V$ of a Markov
chain $M$ whose state is a node $v \in V$ and which, at each step, transitions
to a neighbor of $v$ with probabilities given by $M$.
\end{defn}

A \emph{loop-erased random walk} (or \emph{self-avoiding random walk}) is
obtained by deleting the cycles of a random walk. That is, if in the course of
random walk, we find ourselves at a vertex for a second time the sequence is
deleted to the first visit of the node and we proceed as if it were that first
visit. Evidently, a loop-erased random walk must be finite since any repeating
nodes are deleted.

Some random walk algorithms measure their running time in
comparison to the \emph{cover time} of the graph, which is the expected number
of steps for a random walk (not loop-erased!) to reach all of the vertices the
first time.

% TODO hitting time

\section{Wilson's Algorithm}\label{wilson}
\subsection{Spanning tree with specified root node}
\begin{algorithm}
\caption{Wilson's algorithm for given root}
\label{alg:root}
\textbf{Input: }Graph $G=(V,E)$, Root $r \in V$ \\
\textbf{Output: }Spanning tree $T$ \\
\begin{algorithmic}[1]
\STATE T = \{\}                   // Set of nodes which are in the tree
\STATE next = \{\}                // Map from nodes to their successor
\FOR{ each v in V}
\STATE $u \leftarrow v$
\WHILE{u not in T}\label{walk}
\STATE next[u] $\leftarrow$ samplesuccessor(u)
\STATE u $\leftarrow$ next[u]
\ENDWHILE \label{endwalk}
\STATE $u \leftarrow v$
\WHILE{u not in T} \label{adjoin}
\STATE T.add(u)
\STATE $u \leftarrow $next[u]
\ENDWHILE \label{endadjoin}
\ENDFOR
\STATE \textbf{return} next
\end{algorithmic}
\end{algorithm}

Wilson's algorithm for sampling a spanning tree with a specified root is given
in Algorithm~\ref{alg:root}. It works as follows:
\begin{itemize}
\item Initialize a tree to contain only the root.
\item For each node, perform a random walk until we hit the tree, and add that
path into the tree.
\item Return the resulting tree.
\end{itemize}


We note that the loop on lines~\ref{walk}-\ref{endwalk} implements the
loop-erased random walk in a subtle way.  Specifically, the erasure of a cycle is
\emph{not} a special case, in the sense that if we end up back where we started,
the successor we write down will overwrite the one that was previously there
anyway. This will orphan the rest of the links in the cycle (in the ``next''
array), but this is okay because these links cannot be read by the walk (which only
reads from ``next'' after writing to it) or the tree-adjoining section
(lines~\ref{adjoin}-\ref{endadjoin}) without first being rewritten by the
walk.

\subsection{Spanning tree with unspecified root}
\begin{algorithm}
\caption{Wilson's algorithm with unspecified root}
\label{alg:noroot}
\textbf{Input: }Graph $G=(V,E)$, probability $\epsilon$ \\
\textbf{Output: }Spanning tree $T$ \\
\begin{algorithmic}
\STATE T = \{\}                   // Set of nodes which are in the tree
\STATE next = \{\}                // Map from nodes to their successor
\STATE num\_roots $\leftarrow$ 0
\FOR{$v$ in $V$}
\STATE $u \leftarrow i$
\WHILE{u not in T:}
\IF{chance($\epsilon$)}
\STATE next[u] $\leftarrow$ null
\STATE T.add(u)
\STATE num\_roots += 1
\IF{num\_roots = 2}
\STATE \textbf{return} Fail
\ENDIF
\ELSE
\STATE next[u] $\leftarrow$ samplesucc(u)
\STATE $u \leftarrow $next[u]
\ENDIF
\STATE $u \leftarrow v$
\ENDWHILE
\WHILE{u not in T} \label{adjoin}
\STATE T.add(u)
\STATE $u \leftarrow $next[u]
\ENDWHILE
\ENDFOR
\STATE \textbf{return} next
\end{algorithmic}
\end{algorithm}

Wilson's algorithm for sampling a spanning tree without a specified root is
shown in Algorithm~\ref{alg:noroot}.

\section{Analysis of Wilson's Algorithm}

% Talk about markov chain proof

\section{Implementation}\label{imp}

\printbibliography
\end{document}
